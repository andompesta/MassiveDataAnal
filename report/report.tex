\documentclass{acm_proc_article-sp-sigmod07}

\usepackage{times}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color}
\usepackage{url}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{newclude}
\usepackage{float}
\usepackage{listings}
\usepackage{caption}
%\usepackage[toc,page]{appendix}

%\usepackage{hyperref}
% Pacchetti per poter scrivere con la tastiera italiana
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc} %Aumenta la resa dei caratteri quando si stampa

% Pacchetto per scrivere in italiano
\usepackage[english]{babel}

% Pacchetti matematici vari
% Pacchetti base

\usepackage{makeidx} % Pacchetto per l'indice analitico
\usepackage{mparhack} % Pacchetto che corregge alcuni errori nei magini della pagina 
\usepackage{marginnote} % Pacchetto che permette di scrivere note a margine 
\usepackage{listings} % Pacchetto necessario per scrivere codice sorgente 
\usepackage{braket} % Pacchetto che permette di scrivere tutte le parentesi
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{changepage}
\usepackage{cite}
\usepackage[english]{varioref}

\usepackage[letterpaper]{geometry}
\geometry{top=1.0in, bottom=1.0in, left=1.0in, right=1.0in}

\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{blue},
  commentstyle=\itshape\color{green},
  identifierstyle=\color{black},
  stringstyle=\color{orange},
}

\lstdefinestyle{customasm}{
  belowcaptionskip=1\baselineskip,
  frame=L,
  xleftmargin=\parindent,
  language=[x86masm]Assembler,
  basicstyle=\footnotesize\ttfamily,
  commentstyle=\itshape\color{purple!40!black},
}

\lstset{escapechar=@,style=customc}


\begin{document}
\title{Event summarization from news and tweets correlation}
\author{Cavallari Sandro\\Giglio Marco\\Morettin Paolo
\thanks{We would like to thank Dr. Mikalai Tsytsarau for his valuable advices and for sharing with us his datasets and tools.}}

%
\maketitle
\begin{abstract}
In this report we consider the task of extracting a summary of the main event
that caused a shift on the opinion of Twitter users. This paper presents
several techniques which can be used to analyze these \emph{sentiment shift}
and to find the event which caused them. For each technique presented below
some aspects are taken in account, which are important in the context of a data
mining application, such as scalability and efficiency. A comparison of the
different techniques is shown as well.\end{abstract}   

\section{Introduction}
This work aims to find a summary of the events which caused a \emph{sentiment shift}. 

While event extraction in Natural Language Processing (NLP) techniques are mature,
their performance on tweets inevitably degrades due to the inherent sparsity in
short texts.  Since tweets contain heterogeneous language structures and are at
most 140 characters long, to extract event from tweets is quite tricky: instead
linking tweet to news articles allows us to extract data from well structured
and longer text than tweets.

%Due to the fact that our dataset is composed by labelled tweets, we manage to extract news from the New York Times archive basing the research on tweet's label. Starting form a good Tweet-News classification, allowed us to correlate tweet and news that belong to the same macro-topics avoiding to correlate object of different arguments.

Moreover NLP document compression techniques usually exploits language
dependent methods, whereas we want a methodology as language independent as
possible. 

This paper is structured as follows: in the next chapter we present some works
which are somehow related with the problem we are addressing, then we present a more
formal definition of our problem and some techniques which might be
used to solve it. The techniques which will be later discussed are:
\begin{itemize}
	\item a technique exploiting the \emph{SpaceSaving} algorithm presented in
		\cite{SS};
	\item a technique exploiting \emph{Latent Semantic Indexing} \cite{LSA};
	\item the third one based on the popular \emph{Tf/Idf};
	\item the last (but not least) which take advantage of ngram graphs
		\cite{Ngram}
\end{itemize}
We will discuss each of these showing their advantages and
disadvantages, their efficiency and scalability issues. In addition, we will
show experimental results in order to compare their results and performances.
At the end of the paper we will present
some conclusions and draw some suggestions on how this work may be extended.

\section{Related Work}
\include*{chapters/RelatedWork}

\section{Problem Definition}
\include*{chapters/ProblemDefinition}

\section{Proposed Approach}
\include*{chapters/ProposedApproach}

\section{Experimental Evaluation}
\include*{chapters/ExperimentalEvaluation}

\section{Conclusions}
During the development of this work we found out that working in a blurred environments as the one of the natural language is quite difficult.
Many problem were due to the comparison of the different approach: every algorithm have different output format and normalizing the format in a standard form it's quite challenging. 
Developing an automatic validation system for the results will help a lot the comparison and may produce more objective results.

\subsection*{Future improvement}
\subsubsection*{Redundancy removal}
NGG is a technique developed primary with the goal of \emph{inter-summary} generation: a summarization process that takes into account information already available to the reader. In order to achieve this goal, in a good summary every new sentence must add as less redundant information as possible.
According to the proposal of the NGG developer, implementing the following function can improve the quality of the summary created:
\begin{itemize}
	\item Starting form the first sentence of the summary, compute his NGG representation $G_{sum}$
	\item Remote $G_{sum}$ form the intersection of all the news to summarize ($C_{u}$). The new graph $G_{sum}^{\prime}$ = $G_{sum} \triangle G_{in}$
	\item For every candidate sentence of the news that has not been already used:
	\begin{itemize}
		\item extract its n-gram graph representation $G_{cs}$
		\item keep only $G_{cs}^{\prime} = G_{cs} \triangle C_{in}$, because we want to add sentence with low redundancy
		\item Computing the NVS between $G_{cs}^{\prime}$ and $G_{sum}^{\prime}$ give a \emph{redundancy score}
	\end{itemize}
	\item rank the candidate sentence using as score their NVS value respect $G_{in}$ decreased by the redundancy score
\end{itemize}
this functionality was developed by the author of NGG tool and was used for inter-summary generation. Unfortunately for time reason we weren't able to test his performance in our research, but we think that will produce much more human readable summary with more information.

\subsubsection*{Adaptive time windows}
All the methodologies compute the correlation between news and tweet using a fixed-length time windows. This approach is quite simple and can produce good performance since there are a good number of news in the selected time slice. 

In order to face tricky situation where there are only few news for the selected interval, a more sophisticated approach should adapt the length of the time windows according to the number of news available:
precisely, more news are present in the time period of the sentiment shift and shorter should be the amount of time added to the time windows of CP.
Developing and testing an adaptive time windows can increase the performances of the all the methodologies and is one of the main point for the future improvements.

\begin{thebibliography}{9}
\bibitem{LSA} 
	DEERWESTER, Scott C.. , et al. ``\emph{Indexing by latent semantic analysis}''. JASIS, 1990, 41.6: 391-407.
\bibitem{LSA2} 
	LANDAUER, Thomas K., et al. (ed.). ``\emph{Handbook of latent semantic analysis}''. Psychology Press, 2013.
\bibitem{Gensim}
	ŘEHŮŘEK, Radim, et al. ``\emph{Software framework for topic modelling with large corpora}''. 2010.
\bibitem{Ngram}
	GIANNAKOPOULOS, George, et al. ``\emph{Automatic Summarization from Multiple Documents : N-Gram Graph}''. 2009.
\bibitem{LTN}
	GUO, Weiwei, et al. ``\emph{Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media}''. In: ACL (1). 2013. p. 239-249.
\bibitem{WTMF}
	GUO, Weiwei; DIAB, Mona. ``\emph{Modeling sentences in the latent space}''. In: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012. p. 864-872.
\bibitem{MSC}
	FILIPPOVA, Katja. ``\emph{Multi-sentence compression: Finding shortest paths in word graphs}''. In: Proceedings of the 23rd International Conference on Computational Linguistics. Association for Computational Linguistics, 2010. p. 322-330.
\bibitem{Bifet}
	BIFET, Albert, et al. ``\emph{Detecting Sentiment Change in Twitter Streaming Data}''. WAPA, 2011
\bibitem{SS}
	METWALLY Ahmed, et al. ``\emph{Efficient Computation of Frequent and Top-k
	Elements in Data Streams}'', Lecture Notes in Computer Science Volume 3363, 2005, pp 398-412

\end{thebibliography}
\end{document}
