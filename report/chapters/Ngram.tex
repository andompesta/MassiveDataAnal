N-Gram Graph (NGG) is a NPL tool initially proposed by George Giannakopoulos \cite{Ngram} that uses word or character n-grams in order to achieve documents summarization. The NGG tool basically slices the text in word or character n-grams and then represent them in a graph \emph{G = $\lbrace N,E,L,W\rbrace$} according to the following structure:
\begin{itemize}
	\item N is the set of nodes created for every different n-gram in the text
	\item E represent the edge of the graph; two nodes are connected if they are "close'' or within a \emph{distance window} from each other. The distance windows is denominated as neighbourhood distance
	\item L is a labelling function which assigns labels to every node and every edge (define the size of the n-gram)
	\item W is the weight function which assigns weights to every edge according to the number of times that two n-gram appear close one to the other
\end{itemize}

The big advantage coming from the use of this methodology is language
independence, since it makes no assumption on the underlying languages and
allows text manipulation trough graph operations.

In particular two operation are necessary for our goal:
\begin{itemize}
\item the \textbf{Intersection} operator between two graphs $G_1$ and $G_2$: which returns a resulting graph with only the common edges of $G_1$ and $G_2$ averaging the weights of the original edges assigned as the new edge weights(example in figure \ref{fig:IntersectionOperation})
\item the \textbf{Normalized Value Similarity}[NVS] function that for every n-gram rank, indicating how many of the edges contained in graph $G_i$ are also contained in graph $G_j$, considering also the weights of the matching edges and normalize the result respect the graph size
\end{itemize}
 
\begin{figure*}[htbp]
	\centering
			\begin{subfigure}[G1="aaabaab"]
					{\includegraphics[width=5.5cm,height=6cm]{image/aaab.png}}	
			\end{subfigure}
			\begin{subfigure}[G2="aabac"]
					{\includegraphics[width=4cm,height=6cm]{image/aac.png}}	
			\end{subfigure}
			\begin{subfigure}[Intersection Result]
					{\includegraphics[width=3cm,height=6cm]{image/intersec-res.png}}	
			\end{subfigure}
			\caption[IntersectionOperation]{An example of the intersection operation}
	\label{fig:IntersectionOperation}
\end{figure*}

In particular $NVS(G_i,G_j) = \frac{VS(G_i,G_j)}{SS(G_i,G_j)}$ where:
\begin{equation}
 VS(G_i,G_j)=\frac{\sum e \in G_i \frac{\min(w_e^i, w_e^j)}{\max(w_e^i, w_e^j)}}{\max(\mid G_i \mid, \mid G_j \mid)}
\end{equation}

\begin{equation}
 SS(G_i,G_j)=\frac{\min(\mid G_i \mid, \mid G_j \mid)}{\max(\mid G_i \mid, \mid G_j \mid)}
\end{equation}

Since NGG tools allow to compute both the tweet-news correlation and summary creation, this methodology is split in different section.

\subsubsection*{Tweet-News Correlation}
To compute the correlation between contradiction tweet and news, this methodology use the base idea of exploiting the NVS similarity function as a correlation function: higher the similarity of the tweets n-gram graph representation and the news n-gram representation, higher will be the correlation.

More in detail this procedure perform the following step:
\begin{itemize}
	\item Merge all the tweets corresponding to a CP are in a single document n-gram graph representation ($G_{tw}$).
	\item Compute the news n-gram graph for the news that fall inside a time slots defined by the contradiction point windows increased on both side by a time windows duration ($G_{n}$)
	\item for the news that are inside the time slot, the NVS is computed and represent the correlation value between news and tweet $G_{sim} = NVS(G_n, G_{tw})$
\end{itemize}

The usage of the time windows is needed given that NVS similarity values have no time domain impact, and with out the time windows isn't possible to  discharge news that are distant in time form the correlation computation.

This approach requires a lot of computation, since have to compute the graph representation of all the tweet and for every news inside the time windows compare the tweet with the news text. Instead NGG allow to compute the similarity between text without the usage of grammar information and use this similarity as correlation value: even if has no time relation.


\subsubsection*{Summary Creation}
The second goal of this research is to create a summary of the event that cause the sentiment shift. 

For this purpose we use the news with the highest correlation value obtained at the step before.
Our choice was to limit the summary length to 150 word so there is no need to summarize many document. 
In particulate we select the candidate document $D_c$ to be summarized with the following procedure:
\begin{itemize}
	\item compute the standard deviation($std_{dev}$) between all the correlation value of the document in the selected time windows
	\item add at the set $D_c$ the news mostly correlate and save his correlation value $C_b$
	\item add at the set $D_c$ two more news if their score is inside the interval [$C_b - std_{dev}$, $C_b$]
\end{itemize}
In this way we obtain at most three news to summarize and the intersection of the news will never be empty because we intersect quite similar news.

The summary algorithm perform the following action:
\begin{itemize}
	\item save all the sentence of the candidate news in a set $S_c$; the sentence detection is perform using OpenNLP
	\item create the intersection graph($G_{bn}$) of the candidate news inside $D_c$
	\item for every sentence inside $S_c$ compute the NVS between the sentence n-gram graph and the intersection graph ($NVS( G_{s}, G_{bn} )$)
	\item create the summary using the sentence with the highest NVS value until we reach 150 word
\end{itemize}
this algorithm create a summary composed by the most significant sentences in news with the highest correlation value using a back-loop approach, but is not able to remove redundant sentences. Even so, a summary composed by sentences results to be more human readable than a list of key word.
