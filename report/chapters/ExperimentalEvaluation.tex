\subsection*{Experimental setup}
In order to perform the experimental evaluation we used a tool to automatically
detect sentiment shift in tweets coming from year 2009 and regarding several
topics. We also downloaded news from the \emph{New York Times} and \emph{ABC Australia} on the same topic and spanning on the same period of the tweets.

It's important to remember that tweets have different language structure which need to be normalized so, for cleaning purpose, the following operations were performed on the tweet's text before starting the computation in all the different methodologies:
\begin{itemize}
	\item URLs removal from tweets using a regular expression
	\item conversion from Unicode to ASCII
\end{itemize}

In addition, while parsing the news, we considered the possibility that the
opinion expressed by tweets might be both delayed or in advance with respect to
news (e.g. if a movie becomes popular it is likely that news spread very fast
on twitter, but slower on newspaper; whereas other topics are discussed on
twitter only after related news have been published for several days); thus, we
considered an enlarged time window for news which starts 5 days before the
beginning of the contradiction and last up to 5 days after the end of it.

After that, we manually labelled each contradiction point with the event which
caused it. 

The topics and contradiction points used for the experiments 
are:
\begin{itemize}
 	\item Cern
 	\item FortHood
 	\item Hangover
 	\item Lcross
 	\item Michael Jackson
 	\item SwineFlu
\end{itemize}

A more detailed explanation of those contradiction points is reported in table \ref{tab:setup}. 
For each contradiction point we list:
\begin{itemize}
	\item its symbolical name
	\item the time interval in which it occurred
	\item description of the event which we extracted 
	\item a short label which synthesize it
	\item the number of news present in the contradiction interval $\pm 5$ days
	\item the correlation between news (all-to-all) [IntraNews correlation]
	\item the correlation between news and contradiction tweets [News-Tweets correlation]
\end{itemize}

To compute the \textbf{IntraNews} correlation we developed an algorithm that calculate the NVS between all the possible couple created
using the news inside the time windows. In this approach, the IntraNews correlation, is simply the average between the similarity value.

Likewise, the \textbf{News-Tweets} correlation is the NVS computed between tweets belonging to the contradiction points and 
news of the same time windows.

In detail, the IntraNews correlation is a indicator of how much the news inside the time windows are similar one to the other,
instead the News-Tweets correlation indicate how much the news inside the time windows are similar to the tweet.
Topic with high IntraNews correlation are easy to summarize since every news contains quite the same information;
on the other hand topic with high News-Tweets correlation are topic with low noise.
Both IntraNews and News-Tweets  correlations are calculated using N-gram tools.

\begin{table*}[ht]
	\centering
	\begin{tabularx}{\textwidth}{XX}
	
	\hline
\textbf{Contr. point:} Cern1 & \textbf{Contr. window:} 2009-10-29 - 2009-11-08\\
\multicolumn{2}{>{\setlength{\hsize}{2\hsize}\addtolength{\hsize}{2\tabcolsep}}X} {
	\textbf{Description:} On November 3rd a bird drops a
piece of bread which cause LHC overheating.
} \\
\textbf{Label:} bird accident & \textbf{Number of news in window $\pm 5$ days:} 3 \\
\textbf{IntraNews correlation:} 0.44437 & \textbf{News-Tweets correlation:} 0.3572 \\

	\hline
\textbf{Contr. point:} Cern2 & \textbf{Contr. window:} 2009-11-24 - 2009-12-04\\
\multicolumn{2}{>{\setlength{\hsize}{2\hsize}\addtolength{\hsize}{2\tabcolsep}}X} {
	\textbf{Description:} On November 30th LHC accelerates protons to an
energy of 1.18 TeV, becoming the world most powerful energy particle accelerator.
} \\
\textbf{Label:} power on and record & \textbf{Number of news in window $\pm 5$ days:} 6 \\
\textbf{IntraNews correlation:} 0.25468 & \textbf{News-Tweets correlation:} 0.44832 \\

	\hline
\textbf{Contr. point:} FortHood & \textbf{Contr. window:} 2009-11-02 - 2009-11-06\\
\multicolumn{2}{>{\setlength{\hsize}{2\hsize}\addtolength{\hsize}{2\tabcolsep}}X} {
	\textbf{Description:} On November 5th a US marine kills 13 people.
} \\
\textbf{Label:} shooting & \textbf{Number of news in window $\pm 5$ days:} 70 \\
\textbf{IntraNews correlation:} 0.26553 & \textbf{News-Tweets correlation:} 0.3582 \\

\hline
\textbf{Contr. point:} Hangover & \textbf{Contr. window:} 2009-06-21 - 2009-06-27\\
\multicolumn{2}{>{\setlength{\hsize}{2\hsize}\addtolength{\hsize}{2\tabcolsep}}X} {
	\textbf{Description:} People are enthusiastic about the movie (released on June 5th).
} \\
\textbf{Label:} movie released & \textbf{Number of news in window $\pm 5$ days:} 5 \\
\textbf{IntraNews correlation:} 0.19866 & \textbf{News-Tweets correlation:} 0.37797 \\

\hline
\textbf{Contr. point:} Lcross & \textbf{Contr. window:} 2009-10-31 - 2009-11-08\\
\multicolumn{2}{>{\setlength{\hsize}{2\hsize}\addtolength{\hsize}{2\tabcolsep}}X} {
	\textbf{Description:} NASA publishes preliminary results on the Lcross missions.
	Other results are announced to be released in the next weeks
} \\
\textbf{Label:} preliminary findings & \textbf{Number of news in window $\pm 5$ days:} 0 \\
\textbf{IntraNews correlation:} 0 & \textbf{News-Tweets correlation:} 0 \\

\hline
\textbf{Contr. point:} Jackson1 & \textbf{Contr. window:} 2009-06-24 - 2009-06-27 \\
\multicolumn{2}{>{\setlength{\hsize}{2\hsize}\addtolength{\hsize}{2\tabcolsep}}X} {
\textbf{Description:} The singer dies on June 25th.}\\
\multicolumn{2}{>{\setlength{\hsize}{2\hsize}\addtolength{\hsize}{2\tabcolsep}}X} {
\textbf{Note:} This contradiction window was manually adjusted, since the automatically computed one is not centered on the main event.} \\
\textbf{Label:} Jackson's death & \textbf{Number of news in window $\pm 5$ days:} 143 \\
\textbf{IntraNews correlation:} 0.20704 & \textbf{News-Tweets correlation:} 0.26008 \\

\hline
\textbf{Contr. point:} Jackson2 & \textbf{Contr. window:} 2009-08-23 - 2009-08-29 \\
\multicolumn{2}{>{\setlength{\hsize}{2\hsize}\addtolength{\hsize}{2\tabcolsep}}X} {
	\textbf{Description:} On august 28th another popular
musician, Adam Goldstein, dies. The day after, August 29th is Jackson's
birthday
}\\
\textbf{Label:} Jackson birthday, Goldstein's death & \textbf{Number of news in window $\pm 5$ days:} 48 \\
\textbf{IntraNews correlation:} 0.23583 & \textbf{News-Tweets correlation:} 0.37050 \\

\hline
\textbf{Contr. point:} SwineFlu & \textbf{Contr. window:} 2009-06-19 - 2009-06-23 \\
\multicolumn{2}{>{\setlength{\hsize}{2\hsize}\addtolength{\hsize}{2\tabcolsep}}X} {
	\textbf{Description:} Swine flu is spreading around the world.
}\\
\textbf{Label:} pandemic & \textbf{Number of news in window $\pm 5$ days:} 31 \\
\textbf{IntraNews correlation:} 0.21648 & \textbf{News-Tweets correlation:} 0.20320 \\

\hline
	\end{tabularx}
	\caption{Contradiction points used for experimental evaluation}
	\label{tab:setup}
\end{table*}

Some interesting considerations can be drawn looking at the contradiction points.
First of all, you may notice, that, although our tool for tweet processing detected a \emph{sentiment
shift} in the \emph{Lcross} topic, no news can be found in the selected time period.
Similarly, the amount of news regarding other contradiction points is sometimes
very small; this is particularly true for scientific topics (e.g. \emph{Cern1},
\emph{Cern2}).
Since there are no news in the \emph{Lcross} contradiction point, we will omit it from following
experiments.

\subsection*{Experiments}
Following are reported the main results using the four different approaches previously mentioned.
\subsubsection*{Space Saving evaluation}
Among all the possible SpaceSaving configuration, we chose to split news at
sentence level and to group them 2 by 2, since this seemed to be the best
configuration from some preliminary tests.

The results obtained by running it are reported in table \ref{tab:resultsSS}. For
each contradiction point the three best chunks, as well as the most
common words in the tweets (according to SpaceSaving) and their value.

As you may notice, some sentences are repeated more than once. This can be
justified either by repetition of the same sentence in the same news or by
articles citing previous ones.

\subsubsection*{TF-IDF evaluation}
In table \ref{tab:resultsTfIdf} it is possible to see the results of TF/IDF. For these experiments, we used keyword set of size 30, to produce a single paragraph summary, otherwise it would be to long to be fairly compared to the other methods.

\subsubsection*{LSI evaluation}
LSI as for TF-IDF present single paragraph summaries that are reported on table \ref{tab:resultsLSI}. The choice of the number of dimensions of the LSI space is non-trivial. For small dataset like ours, a number between 50 and 100 has proven to be optimal\cite{LSA2}. At the same time, for a more specific discrimination in an homogeneous topic a larger number is suggested. For those reasons we used a 200 dimensions vectorial space.

\subsubsection*{N-Gram Graph evaluation}
Table \ref{tab:resultsNGG} reports the identified results expressing, among the others, the correlation score of the best news. It's the maximum correlation value between a news and the contradiction tweet.
It's important to point out that, for presentation reasons, the summary reported is smaller respect the one created by the NGG methodologies and all the experiment reported where conduced with the default setting suggested by George Giannakopoulos of rank=3 and neighbourhood distance = 3

\include{chapters/results/SS}

\include{chapters/results/TF-Idf}

\include{chapters/results/LSI}

\include{chapters/results/NGG}

\subsection*{Performance}
For the first three approaches time performance is not really relevant since it's all experiments were performed in few seconds or less, making them really suitable for real time processing. N-Gram Graph, on the other hand, is more resources consuming, hence we made more detailed investigations regarding its  response time.

\subsubsection*{N-Gram Graph }
All the experiment about N-Gram graph where conduced on a 2,4 GHz Intel Core 2 Duo with 4 GB 1067 MHz DDR3 RAM memory.
 All the code was compiled with intellij IDEA on Java 1.8 language. 
The reported test where conduced on Cern topic data set, since is the one that have multiple contradiction points and allowed us to test the methodology changing the following parameter:
\begin{itemize}
	\item rank of the graph
	\item neighbourhood distance[ND]
\end{itemize}


the computational time respect the neighbourhood distance for different rank values is reported in figure \ref{fig:et-fixed-10days-r-rank}.
It can be noticed, for all ranks, there is an exponential behaviour in relation to the neighbourhood distance. 
Increasing ND cause an exponential increase of the edges number that a graph contains, so the computation time became exponentially longer.

\begin{figure}[htbp]
	\centering
			{\includegraphics[width=8.5cm,height=7cm]{image/win_2_ranks.pdf}}	
		\caption[et-fixed-10days-r-rank]{Execution time for different rank value}
	\label{fig:et-fixed-10days-r-rank}
\end{figure} 

Figure \ref{fig:et-fixed-10days-r-nd} shows how the computation time changes in respect to the rank value at different neighbourhood distance;
increasing the rank, the computational time is quite the same.
Apparently, the rank can mitigate the neighbourhood distance effect.

The computation time for of the 2-gram diverge, instead 3-gram became more similar to the 4-gram behaviour. 
Probably because computing the similarity between 2-gram graph is an operation quite expensive since they have many edges to check.
Instead the neighbourhood distance is a less impactive parameter; a value as 3 o 4 requires quite the same computational time.

\begin{figure}[htbp]
	\centering
			{\includegraphics[width=8.5cm,height=7cm]{image/win_2_ndists.pdf}}	
					\caption[et-fixed-10days-r-nd]{Execution time for different neighbourhood distance value}
	\label{fig:et-fixed-10days-r-nd}
\end{figure} 

Concerning the evaluation of the summary, the neighbourhood distance factor has little influence on the summary since changing its value just produce a swapping on the order of the sentence in the summary.

Always using Cern dataset, we notice that for a rank of 3 we obtain the following results:

\begin{enumerate}
	\item CERN said the latest incident was minor and did not affect attempts to restart the accelerator later this month following repairs.
	\item 'The bird escaped unharmed but lost its bread,'CERN said in a statement.
	\item Bits of a French loaf dropped on an external electrical power supply caused a short circuit last week, triggering failsafe devices that shut down part of the cooling system of the giant experiment to probe the secrets of the universe.
	\item The bird was believed to be an owl.
	\item The European Organisation for Nuclear Research (CERN) says the system was restored several hours after the incident last week while the multi-billion-dollar Large Hadron Collider was barely affected.
	\item The 27 kilometre-long particle collider, which runs in a circular tunnel under the French-Swiss border near Geneva, has been plagued by problems since it was briefly started in September 2008.
\end{enumerate}

instead for a rank = 6 we obtain the summary:

\begin{enumerate}
	\item CERN said the latest incident was minor and did not affect attempts to restart the accelerator later this month following repairs.
	\item Bits of a French loaf dropped on an external electrical power supply caused a short circuit last week, triggering failsafe devices that shut down part of the cooling system of the giant experiment to probe the secrets of the universe.
	\item The European Organisation for Nuclear Research (CERN) says the system was restored several hours after the incident last week while the multi-billion-dollar Large Hadron Collider was barely affected.
	\item 'The bird escaped unharmed but lost its bread,'CERN said in a statement. 
	\item The bird was believed to be an owl.
	\item The 27 kilometre-long particle collider, which runs in a circular tunnel under the French-Swiss border near Geneva, has been plagued by problems since it was briefly started in September 2008.
\end{enumerate}

As conclusion we can say that the rank has just a little effect of the order of the sentence, but not on the news-tweet correlation.
Knowing that neighbourhood distance strongly affect the computational time and slightly change the summary; it is reasonable to use a ND between 3 and 4 as suggested by the N-gram graph creator.


%Figure \ref{fig:et-fixed-r3-winds} report the variation of computation time for fixed rank value(3). The shape of the computation time in this case is always the same: exponential respect the ND. 
%Notice that the max computation time always increase with the increasing of the windows size, since the algorithm have to compute the similarity between more news. In this way we can derive that the number of news, inside the time windows, affect the computation time in a almost linear way.
