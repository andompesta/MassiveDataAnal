\subsection*{Experimental setup}
In order to perform the experimental evaluation we used a tool to automatically
detect sentiment shift in tweets coming from year 2009 and regarding several
topics. We also downloaded news from the \emph{New York Times} and \emph{ABC
Australia} on the same topic and spanning on the same period of the tweets.

It's important to remember that tweet have different language structure that need to be normalized, so for cleaning purpose, the following operations were performed on the tweet text before starting the computation of all the different methodologies:
\begin{itemize}
	\item URLs removal from tweets using a regular expression
	\item conversion from Unicode to ASCII
\end{itemize}

In addition, while parsing the news, we considered the possibility that the
opinion expressed by tweets might be both delayed or in advance with respect to
news (e.g. if a movie becomes popular it is probable that news spread very fast
on twitter, but slower on newspaper, whereas other topics are discussed on
twitter only after news about them came out for several days). Thus, we
considered a enlarged time window for news which starts 5 days before the
beginning of the contradiction and last up to 5 days after the end of it.

After that, we manually labelled each contradiction point with the event which
caused it. The topics, contradiction points and events used for the experiments
are showed in table \ref{tab:setup}

\begin{table*}
	\centering
	\begin{tabularx}{\textwidth}{|l|l|X|}
	\hline
	Contr. point 	& Contr. window 			& Event \\
	\hline 
	Cern1			& 2009-10-29 - 2009-11-08 	& On November 3rd a bird drops a
piece of bread which cause LHC overheating. \\
	Cern2			& 2009-11-24 - 2009-12-04	& On November 30th LHC accelerates protons to an
energy of 1.18 TeV, becoming the world most powerful energy particle
accelerator\\
	FortHood 		& 2009-11-02 - 2009-11-06	& On November 5th a US marine
kills 13 people\\
	HangOver		& 2009-06-21 - 2009-06-27	& undetermined or movie released
	on June 5th\\
	Lcross			& 2009-10-31 - 2009-11-08	& Preliminary findings from
	Lcross. Others announced\\
	Jackson1		& 2009-06-19 - 2009-06-25	& On June 25th Michael Jackson
dies\\
	Jackson2		& 2009-08-23 - 2009-08-29	& On august 28th another popular
musician, Adam Goldstein, dies. The day after, August 29th is Jackson's
birthday\\
	SwineFlu		& 2009-06-19 - 2009-06-23	& The swine flu is recognized as
	a pandemic\\
	\hline
	\end{tabularx}
	\caption{Contradiction points used for experimental evaluation}
	\label{tab:setup}
\end{table*}

\subsection*{Experiments}
\subsubsection*{SpaceSaving evaluation}
The results achieved using the \emph{SpaceSaving} methodology with a maximum
error of five days on the contradiction window are shown in table
\ref{tab:resultsSS}.
\include{chapters/results/SS}

\subsubsection*{LSI evaluation}
%\begin{table*}
%	\centering
%	\begin{tabularx}{\textwidth}{|l|X|}
%		\hline
%		Contr. point & Output \\
%		Cern1		&
%		Cern2		&
%		FortHood	&
%		HangOver	&
%		Lcross		&
%		Jackson1	&
%		Jackson2	&
%		SwineFlu	&
%	\end{tabularx}
%	\caption{Results achieved using LSI}
%	\label{tab:resultsLSI}
%\end{table*}


\subsubsection*{Tf/idf evaluation}
%\begin{table*}
%	\centering
%	\begin{tabularx}{\textwidth}{|l|X|}
%		\hline
%		Contr. point & Output \\
%		Cern1		&
%		Cern2		&
%		FortHood	&
%		HangOver	&
%		Lcross		&
%		Jackson1	&
%		Jackson2	&
%		SwineFlu	&
%	\end{tabularx}
%	\caption{Results achieved using Tf/idf}
%	\label{tab:resultsTFIDF}
%\end{table*}

\subsubsection*{Ngram Graph evaluation}
As for the other methods for the NGG computation we used a time windows of five days and in table \ref{} are reported the obtained results

%\begin{table*}
%	\centering
%	\begin{tabularx}{\textwidth}{|l|X|}
%		\hline
%		Contr. point & Output \\
%		Cern1		&
%		Cern2		&
%		FortHood	&
%		HangOver	&
%		Lcross		&
%		Jackson1	&
%		Jackson2	&
%		SwineFlu	&
%	\end{tabularx}
%	\caption{Results achieved using ngram graphs}
%	\label{tab:resultsNGG}
%\end{table*}

All the experiment about Ngram graph where conduced on a 2,4 GHz Intel Core 2 Duo with 4 GB 1067 MHz DDR3 RAM memory.
 All the code was compiled with intellij IDEA on Java 1.8 language. 
The reported test where conduced on Cern topic data set, since is the one that have multiple contradiction points and allowed us to test the methodology changing the following parameter:
\begin{itemize}
	\item rank of the graph
	\item neighbourhood distance[ND]
	\item time windows
\end{itemize}

In figure \ref{fig:et-fixed-10days-r-rank} is reported the computational time respect the neighbourhood distance for different rank values. 
It is easy to notice how for all the rank there is an exponential behaviour respect to the neighbourhood distance. 
Increasing ND will increase exponentially the number of edges that a graph contains, so the computation will became exponentially longer.
Since neighbourhood distance strongly affect the computational time and have no big difference on the summary obtained, is reasonable to use a ND between 3 and 4.

\begin{figure*}[htbp]
	\centering
			\begin{subfigure}[rank = 2]
					{\includegraphics[width=5cm,height=5cm]{image/win_2_rank_2.pdf}}	
			\end{subfigure}
			\begin{subfigure}[rank = 3]
					{\includegraphics[width=5cm,height=5cm]{image/win_2_rank_3.pdf}}	
			\end{subfigure}
			\begin{subfigure}[rank = 4]
					{\includegraphics[width=5cm,height=5cm]{image/win_2_rank_4.pdf}}	
			\end{subfigure}
			\caption[et-fixed-10days-r-rank]{Execution time for different rank value}
	\label{fig:et-fixed-10days-r-rank}
\end{figure*} 

In figure \ref{fig:et-fixed-10days-r-nd} is possible to see how the computation time change respect to the rank value at different neighbourhood distance. 
Surprisingly for a neighbourhood distance = 3 the computation time with a n-gram graph of rank=3 is slightly higher respect to a rank=2. 
That is quite strange since slicing a text in two-gram should produce more node that slicing a text in three-grams.
Probably with rank = 2 and ND = 3; near grams are part of the same word creating graph easy to compute.
This scenario produce a smaller computation time in a two-gram graph that a three-gram graph where we have more distinct node and so edge to check.

Is possible to notice how for bigger ND values, the computation time for of the 2-gram diverge, instead 3-gram became more similar at the 4-gram behaviour. Probably because computing the similarity between 2-gram graph is an operation quite expensive since they have many edges to check.

\begin{figure*}[htbp]
	\centering
			\begin{subfigure}[neighbourhood distance = 3]
					{\includegraphics[width=5cm,height=5cm]{image/win_2_ndist_3.pdf}}	
			\end{subfigure}
			\begin{subfigure}[neighbourhood distance = 4]
					{\includegraphics[width=5cm,height=5cm]{image/win_2_ndist_4.pdf}}	
			\end{subfigure}
			\begin{subfigure}[neighbourhood distance = 5]
					{\includegraphics[width=5cm,height=5cm]{image/win_2_ndist_5.pdf}}	
			\end{subfigure}
			\begin{subfigure}[neighbourhood distance = 6]
					{\includegraphics[width=5cm,height=5cm]{image/win_2_ndist_6.pdf}}	
			\end{subfigure}
			\caption[et-fixed-10days-r-nd]{Execution time for different neighbourhood distance }
	\label{fig:et-fixed-10days-r-nd}
\end{figure*} 


%Figure \ref{fig:et-fixed-r3-winds} report the variation of computation time for fixed rank value(3). The shape of the computation time in this case is always the same: exponential respect the ND. 
%Notice that the max computation time always increase with the increasing of the windows size, since the algorithm have to compute the similarity between more news. In this way we can derive that the number of news, inside the time windows, affect the computation time in a almost linear way.


