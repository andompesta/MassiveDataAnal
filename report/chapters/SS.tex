\textbf{SpaceSaving} \cite{SS} is an algorithm which was first presented by Metwally et al.
in 2005 to efficiently compute the most frequent terms in a data stream. It
allows the user to find $k$ words which are among the most frequent in the given
stream. 
Although this is a heuristic algorithm whose resultâ€™s correctness is not guaranteed, SpaceSaving is able to specify the upper bound of the error for each word presented as output.

The classic implementation is based on a fixed size list of tuple 
\begin{displaymath}
	(word, occurrencies, error)
\end{displaymath}
The stream is read word by word and at any time one of the following condition
is verified:
\begin{enumerate}
	\item \label{SS-among-frequent}
		the word is already in the list of frequent terms
	\item \label{SS-list-not-full}
		the word is not among the frequent terms, but the list is not full
	\item \label{SS-list-full}
		the word is not among the frequent terms and the list is full
\end{enumerate}
If the first condition is verified, then the algorithm will proceed incrementing
the number of occurrencies of that term; if condition \ref{SS-list-not-full} is
verified, instead, we add that word to the list of frequent terms with number of
occurrencies 1 and number of errors 0; if condition \ref{SS-list-full} is
verified, then we scan the list for the term with the lowest number of
occurrencies and replace it with the new word. When this replacement takes place
we set the number of errors equal to the number of occurrencies of the word we
just replaced, then we increment the number of occurrencies.

Our implementation is slightly different from the
classic one described above, since we test the word against a list of \emph{stop words}
and we added a hashmap to avoid useless replacement. The pseudo code representing
our implementation is the following:

\begin{algorithmic}
	\IF{word not in StopWordList} 
	\STATE H = hash(word);
	\STATE hashmap[H]++;
	\IF{hashmap[H] > min(occurrencies)}
	    \STATE minElement = less frequent term
    	\STATE minElement[term] = word
    	\STATE minElement[error] = minElement[occ]
	    \STATE minElement[occ]++
	\ENDIF
	\ENDIF
\end{algorithmic}

The output of this algorithm is a couple $(word, value)$ where the value is
computed as 
\begin{displaymath}
	value = occurrencies - error
\end{displaymath}
In order to reduce the noise we chose to use only couples where the value is
above a certain threshold; the threshold value is identified empirically.

\subsubsection*{Description of the methodology}
We have seen how we exploit the SpaceSaving algorithm to obtain the list of
frequent terms in a text, and their values. 

To accomplish our task, we ran the program described above on the set of tweets inside a
contradiction point, thus obtaining the list of frequent terms within the
contradiction. After this process, we read all the news which were published in the same time
period, we split them into chunks (the description of the possible methods to
perform this split can be found in the next section) and compute the score for
each chunk as
\begin{displaymath}
	newsScore = \sum_{w \in W} value(w)
\end{displaymath}
where $W$ is the list of words in the chunk and the value is null if that term is
not in the \emph{frequent term list}.

The pseudo code of the scoring function is the following:
%function NewsScore(WordValueList) {
\begin{algorithmic}
\STATE newsList = news in contr. window
\STATE bestSentence = ``''
\STATE bestScore = 0
\FOR{news in newsList}
	\STATE chunkList.append(chunks from news)
\ENDFOR
\FOR{chunk in chunkList}
	\STATE score = 0
	\FOR{word in chunk}
		\IF{word in WordValueList}
			\STATE score += WordValueList[word][``value'']
		\ELSE \IF{score > bestScore}
			\STATE bestScore = score
			\STATE bestSentence = chunk
		\ENDIF \ENDIF
	\ENDFOR
\ENDFOR
\end{algorithmic}
The sentence with the highest score is selected as the one causing the shift and
it is presented as output to the user.

This approach revealed to be very fast and from its formulation we can see that
it can be applied incrementally and in real time. As a drawback, we must say that it does not
take in account the background coming from the topic, hence it is not able to
discern whether a given frequent term is informative or not.

E.g. running this program on the Obama topic it is likely that the most
frequent terms will be ``President'', `Barack'', ``Obama'', ``USA'', hence a
news containing more repetitions of these words will have a high score, even if
those words are not informative at all.

\subsubsection*{Implementation}
The chunks we discussed in previous section can be entire paragraph or blocks of
$n$ consecutive sentences. A well known library, know as \emph{nltk}\cite{nltk} has been used in order to
split news articles at sentence level. Using this library was needed since we wanted
to prevent our program from splitting sentences in the wrong point (i.e.
acronyms and abbreviations have to be taken in account).
