We used a bag-of-words approach exploiting a technique called Latent Semantic Analysis (also known as Latent Semantic Indexing), formalized by Scott Deerwester et al. in 1990\cite{LSA}. LSA is an information retrieval statistical approach, that can be used to find semantic similarities between sets of documents. It takes advantage of Singular Value Decomposition on a vector space of term frequencies in order to create, given a set of documents, an \emph{index} matrix, that can be queried with other documents to find the most similar one.
\\
As for the TF-IDF approach, it is assumed to deal with news articles composed by separated paragraphs.
\\
Given a topic $T$ and the number of dimensions of the LSA space $dim$, our implementation returns the $p$ most similar paragraphs to each sentiment shift. The training of the model is performed on the whole corpus of tweets of the topic, while the similarity of a paragraph to the shift is computed as the sum of the similarities w.r.t. each tweet belonging to the latter.
\\\\
\begin{algorithmic}
\STATE \textbf{LSA Method}($T$,$dim$,$p$)
\STATE
\STATE $LSA$.train($Tweets_T$,$dim$)
\STATE
\FORALL {$S \in Shifts_T$}
	\FORALL {$Article \in News_{T,S}$}
		\FORALL {$Paragraph \in Article$}
			\STATE $simVector$ $\leftarrow$ $LSA$.index($Tweets_{T,S}$,$Paragraph$)
			\STATE $similarity$($Paragraph$) $\leftarrow$ $\sum_{x \in simVector} x $
		\ENDFOR
	\ENDFOR
	\STATE $summaries$($S$) $\leftarrow$ the $p$ paragraphs with higher $similarity$

\ENDFOR
\STATE
\RETURN $summaries$

\end{algorithmic}

\subsubsection*{Preprocessing}
During the preprocessing, every document is filtered and converted from a string to a bag-of-word representation. The following steps are done in this phase:
\begin{enumerate}
\item The punctuation characters are substituted with a whitespace.
\item The string is tokenized.
\item Stopwords and tokens appearing less than $t$ times in the document are removed.
\end{enumerate}

\subsubsection*{LSA and candidate news selection}
Once the documents are filtered and transformed in a bag-of-words representation, a dictionary is created containing the association between tokens and a positive integer identifier. This is done in order to convert on-the-fly each document $D$ in a vector of tuples $(i,n(D,i))$ where $n(D,i)$ is the number of occurrences of the token $i$ in the document $D$. The LSA model is then trained with all the news available for the given topic. Given a sentiment shift over a time interval $[T_b,T_e]$, the news can be selected using a fixed size window $[T_b - c,T_e]$ or a more sophisticated approach that considers the density of the tweets inside the shift. The latter seems reasonable since news that correlates with a shift are likely to be near the shift the more the \emph{hype} increases.

\subsubsection*{News scoring and summarization}
With the candidate news a matrix index is then created and the document representing the shift is compared. This operation produces a vector of similarities of the shift with the candidate news. These scores do not take into account the hype and the temporal distance of the news w.r.t. the sentiment shift. Unfortunately, we didn't had time to investigate this aspect and find a reasonable weighting function on those parameters. The candidate news passing a threshold are considered to be correlated with the shift, so for each of them the list of the $k$ words with higher TF-IDF values are returned.

\subsubsection*{Implementation}
The methodology described above was implemented in python 2.7 using a library called gensim\cite{Gensim}. It was possible to store all the preprocessed data for a single topic on main memory and perform all the computation in a single pass. The choice of the number of dimensions of the LSA space is non-trivial. For small dataset like ours, a number between 50 and 100 has proven to be optimal\cite{LSA2}. At the same time, for a more specific discrimination in an homogeneous topic a larger number is suggested. For those reasons we used a 200 dimensions vectorial space.
