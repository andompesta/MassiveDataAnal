We used a bag-of-words approach exploiting a technique called Latent Semantic Analysis (also known as Latent Semantic Indexing), formalized by Scott Deerwester et al. in 1990\cite{LSA}. LSA is a statistical approach, used to find semantical similarities between sets of documents. It takes advantage of Singular Value Decomposition on a vector space of term frequencies in order to create, given a set of documents, an \emph{index} matrix, that can be queried with other documents to find the most similar one. In order to find correlations between the sentiment shifts and the news, our approach works as follows:
\begin{itemize}
\item All the tweets corresponding to a shift are merged in a single document.
\item Every document is preprocessed and converted in the bag-of-word representation.
\item The LSA model is trained on the whole set of news for the given topic.
\item The set of candidate news to be tested is chosen accordingly to a window, and it's used to create an index.
\item The news are sorted in order of similarity, optionally weighted by a factor considering the time distance of the news from the shift.
\item All the candidate news over a certain threshold are considered to be correlated and a set of words calculated with TF-IDF is returned.
\end{itemize}

\subsubsection*{Preprocessing}
During the preprocessing, every document is filtered and converted from a string to a bag-of-word representation. The following steps are done in this phase:
\begin{enumerate}
\item (Optional) URLs removal using a regular expression.
\item (Optional) conversion from Unicode to ASCII.
\item The punctuation characters are substituted with a whitespace.
\item The string is tokenized.
\item Stopwords and tokens appearing less than $t$ times in the document are removed.
\end{enumerate}

\subsubsection*{LSA and candidate news selection}
Once the documents are filtered and transformed in a bag-of-words representation, a dictionary is created containing the association between tokens and a positive integer identifier. This is done in order to convert on-the-fly each document $D$ in a vector of tuples $(i,n(D,i))$ where $n(D,i)$ is the number of occurrences of the token $i$ in the document $D$. The LSA model is then trained with all the news available for the given topic. Given a sentiment shift over a time interval $[T_b,T_e]$, the news can be selected using a fixed size window $[T_b - c,T_e]$ or a more sophisticated approach that considers the density of the tweets inside the shift. The latter seems reasonable since news that correlates with a shift are likely to be near the shift the more the \emph{hype} increases.

\subsubsection*{News scoring and summarization}
With the candidate news a matrix index is then created and the document representing the shift is compared. This operation produces a vector of similarities of the shift with the candidate news. These scores do not take into account the hype and the temporal distance of the news w.r.t. the sentiment shift. Unfortunately, we didn't had time to investigate this aspect and find a reasonable weighting function on those parameters. The candidate news passing a threshold are considered to be correlated with the shift, so for each of them the list of the $k$ words with higher TF-IDF values are returned.

\subsubsection*{Implementation}
The methodology described above was implemented in python 2.7 using a library called gensim\cite{Gensim}.
Even if the time complexity is theoretically $O(2^m)$ for the compilation of a regular expression of length $m$, in practice step 1 is very fast due to the simplicity of the one matching the URLs. Every other step takes linear time w.r.t. the document length. Steps 2 and 3 take linear time w.r.t. the number of characters in the document, steps 4 and 5 take linear time w.r.t. the number of words in the document. Every document is preprocessed at runtime and stored in main memory