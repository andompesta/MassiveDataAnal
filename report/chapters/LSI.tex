We used a bag-of-words approach exploiting a technique called Latent Semantic Analysis (also known as Latent Semantic Indexing), formalized by Scott Deerwester et al. in 1990\cite{Deerwester}. LSA is a statistical approach, used to find semantical similarities between sets of documents. It takes advantage of Singular Value Decomposition on a vector space of term frequencies in order to create, given a set of documents, an \emph{index} matrix, that can be queried with other documents to find the most similar one. In order to find correlations between the sentiment shifts and the news, our approach works as follows:
\begin{itemize}
\item All the tweets corresponding to a shift are merged in a single document.
\item Every document is preprocessed and converted in the bag-of-word representation.
\item The LSI model is trained on the whole set of news for the given topic.
\item The set of candidate news to be tested is chosen accordingly to a window, and it's used to create an index.
\item The news are sorted in order of similarity, possibily weighted by a factor considering the time distance of the news from the shift.
\item All the candidate news over a certain threshold are considered to be correlated and a set of words calculated with TF-IDF is returned.
\end{itemize}

\subsection{Preprocessing}
During the preprocessing, every document is filtered and converted from a string to a bag-of-word representation. The following steps are done in this phase:
\begin{itemize}
\item (Optional) URLs removal.
\item (Optional) conversion from Unicode to ASCII.
\item The punctuation characters are substituted with a whitespace.
\item The string is tokenized.
\item Stopwords and tokens appearing less than \math{t} times in the document are removed.
\end{itemize}

\subsection{LSI and candidate news selection}
Once the documents are filtered and transformed in a bag-of-words representation, a dictionary is created containing the association between tokens and a positive integer identifier. This is done in order to convert on-the-fly each document \math{D} in a vector of tuples \math{(i,n(D,i))} where \math{n(D,i)} is the number of occurrencies of the token \math{i} in the document \math{D}.