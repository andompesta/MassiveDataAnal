We used a bag-of-words approach exploiting a technique called LSI formalized by Scott Deerwester et al. in 1990\cite{LSA}. LSI is an information retrieval statistical approach, that can be used to find semantically similarities between sets of documents. It takes advantage of \emph{Singular Value Decomposition} on a vector space of term frequencies in order to create, given a set of documents, an \emph{index} matrix, that can be queried with other documents to find the most similar one. In order to find correlations between the sentiment shifts and the news articles, our approach works as follows:
\begin{itemize}
\item All the tweets corresponding to a CP are merged in a single document.
\item Every document is preprocessed and converted in the bag-of-word representation.
\item The LSI model is trained on the whole set of news for the given topic.
\item The set of candidate news to be tested is chosen according to a window, and  used to create an index.
\item The news are sorted in order of similarity, optionally weighted by a factor considering the time distance of the news from the CP.
\item All the candidate news over a certain threshold are considered to be correlated and a set of words calculated with TF-IDF is returned.
\end{itemize}

\subsubsection*{Preprocessing}
During the preprocessing, every document is filtered and converted from a string to a bag-of-word representation. The following steps are done in this phase:
\begin{enumerate}
\item The punctuation characters are substituted with a white space.
\item The string is tokenized.
\item Stop words and tokens appearing less than $t$ times in the document are removed.
\end{enumerate}

\subsubsection*{LSI and candidate news selection}
Once the documents are filtered and transformed in a bag-of-words representation, a dictionary is created containing the association between tokens and a positive integer identifier. 
This is done in order to convert on-the-fly each document $D$ in a vector of tuples $(i,n(D,i))$ where $n(D,i)$ is the number of occurrences of the token $i$ in the document $D$. 
The LSI model is then trained with all the news available for the given topic.
Given a sentiment shift over a time interval $[T_b, T_e]$, the news can be selected using a fixed size window $[T_b - c,T_e]$ or a more sophisticated approach that considers the density of the tweets inside the shift.
%The latter seems reasonable since news that correlates with a shift are likely to be temporarely close to a shift dependig on the increase of the \emph{hype}.

\subsubsection*{News scoring and summarization}
With the news belonging to the contradiction window a matrix index is created and this matrix index used in comparison with the contradiction tweets.
This operation produces a vector of similarities of the shift with the candidate news.
These scores do not take into account the hype and the temporal distance of the news w.r.t. the sentiment shift.
%The candidate news passing a threshold are considered to be correlated with the shift, so for each of them the list of the $k$ words with higher TF-IDF values are returned.

\subsubsection*{Implementation}
The methodology described above was implemented a library called gensim \cite{Gensim}. 
It was possible to store all the preprocessed data for a single topic on main memory and perform all the computation in a single pass.
The choice of the number of dimensions of the LSI space is non-trivial.
For small dataset like ours, a number between 50 and 100 has proven to be optimal\cite{LSA2}.
At the same time, for a more specific discrimination in an homogeneous topic a larger number is suggested.
For those reasons we used a 200 dimensions vectorial space.
